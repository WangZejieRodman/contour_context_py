#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆåˆ†å±‚æ•°é‡åˆ†æå®éªŒ
æ·±å…¥åˆ†æå±‚æ•°å˜åŒ–å¯¹ç‰¹å¾æå–å’Œå¬å›ç‡çš„å½±å“
æ‰©å±•åˆ°1-20å±‚ï¼ŒåŒ…å«è¯¦ç»†çš„è½®å»“ã€BCIã€ç‰¹å¾ç»Ÿè®¡åˆ†æ
"""

import os
import sys
import time
import re
import subprocess
from typing import Dict, List, Tuple, Any
import tempfile
import shutil
import statistics
import numpy as np
import json


class ComprehensiveLayersAnalysisExperiment:
    """ç»¼åˆåˆ†å±‚æ•°é‡åˆ†æå®éªŒç±»"""

    def __init__(self):
        self.base_script = "contour_chilean_åœºæ™¯è¯†åˆ«_ä¸ç›¸åŒæ—¶æ®µ100-190_ä¸åŠ æ—‹è½¬.py"
        self.base_types = "contour_types.py"
        self.results = []

        # æ‰©å±•åˆ°1-20å±‚çš„å®Œæ•´æµ‹è¯•
        self.experiments = self._generate_layer_experiments(1, 2)

        # ç‰¹å¾åˆ†æçš„è¯¦ç»†é…ç½®
        self.contour_size_bins = [
            (1, 5, "æå°è½®å»“"),
            (6, 10, "å°è½®å»“"),
            (11, 20, "ä¸­å°è½®å»“"),
            (21, 50, "ä¸­ç­‰è½®å»“"),
            (51, 100, "å¤§è½®å»“"),
            (101, float('inf'), "æå¤§è½®å»“")
        ]

        self.eccentricity_bins = [
            (0.0, 0.3, "è¿‘åœ†å½¢"),
            (0.3, 0.6, "æ¤­åœ†å½¢"),
            (0.6, 0.8, "é•¿æ¤­åœ†"),
            (0.8, 1.0, "æé•¿æ¤­åœ†")
        ]

    def _generate_layer_experiments(self, min_layers: int, max_layers: int) -> List[Dict]:
        """ç”Ÿæˆ1-20å±‚çš„å®éªŒé…ç½®"""
        experiments = []

        for num_layers in range(min_layers, max_layers + 1):
            if num_layers == 1:
                # 1å±‚ç‰¹æ®Šæƒ…å†µ
                lv_grads = [0.0, 5.0]
                q_levels = [0]
                dist_layers = [0]
                weights = [1.0]
            else:
                # å¤šå±‚æƒ…å†µï¼šå‡åŒ€åˆ†å‰²0-5m
                lv_grads = []
                for i in range(num_layers + 1):
                    height = 5.0 * i / num_layers
                    lv_grads.append(round(height, 3))

                q_levels = list(range(num_layers))
                dist_layers = list(range(num_layers))

                # ç”Ÿæˆæƒé‡ï¼ˆä¸­é—´å±‚çº§æƒé‡æ›´é«˜ï¼‰
                weights = self._generate_layer_weights(num_layers)

            experiment = {
                "name": f"{num_layers}layers",
                "layers_count": num_layers,
                "layer_interval": 5.0 / num_layers,
                "lv_grads": lv_grads,
                "q_levels": q_levels,
                "dist_layers": dist_layers,
                "weights": weights
            }

            experiments.append(experiment)

        return experiments

    def _generate_layer_weights(self, num_layers: int) -> List[float]:
        """ç”Ÿæˆå±‚çº§æƒé‡"""
        if num_layers <= 3:
            return [1.0 / num_layers] * num_layers

        weights = []
        for i in range(num_layers):
            # ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒï¼Œä¸­é—´å±‚çº§æƒé‡æ›´é«˜
            center = (num_layers - 1) / 2
            distance = abs(i - center) / (num_layers / 2)
            weight = np.exp(-distance * distance)
            weights.append(weight)

        # å½’ä¸€åŒ–
        total = sum(weights)
        return [w / total for w in weights]

    def backup_files(self):
        """å¤‡ä»½åŸå§‹æ–‡ä»¶"""
        print("å¤‡ä»½åŸå§‹æ–‡ä»¶...")
        if os.path.exists(self.base_script):
            shutil.copy2(self.base_script, f"{self.base_script}.backup")
        if os.path.exists(self.base_types):
            shutil.copy2(self.base_types, f"{self.base_types}.backup")

    def restore_files(self):
        """æ¢å¤åŸå§‹æ–‡ä»¶"""
        print("æ¢å¤åŸå§‹æ–‡ä»¶...")
        if os.path.exists(f"{self.base_script}.backup"):
            shutil.copy2(f"{self.base_script}.backup", self.base_script)
            os.remove(f"{self.base_script}.backup")
        if os.path.exists(f"{self.base_types}.backup"):
            shutil.copy2(f"{self.base_types}.backup", self.base_types)
            os.remove(f"{self.base_types}.backup")

    def modify_config_files(self, experiment: Dict):
        """ä¿®æ”¹é…ç½®æ–‡ä»¶"""
        print(f"ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸º: {experiment['name']} ({experiment['layers_count']}å±‚)")

        # ä¿®æ”¹ contour_types.py
        self._modify_contour_types(experiment)

        # ä¿®æ”¹ä¸»ç¨‹åºæ–‡ä»¶
        self._modify_main_script(experiment)

    def _modify_contour_types(self, experiment: Dict):
        """ä¿®æ”¹ contour_types.py æ–‡ä»¶"""
        with open(self.base_types, 'r', encoding='utf-8') as f:
            content = f.read()

        # ä¿®æ”¹ DIST_BIN_LAYERS
        dist_layers_str = str(experiment['dist_layers'])
        content = re.sub(
            r'DIST_BIN_LAYERS = \[.*?\]',
            f'DIST_BIN_LAYERS = {dist_layers_str}',
            content
        )

        # ä¿®æ”¹ LAYER_AREA_WEIGHTS
        weights_str = '[' + ', '.join([f'{w:.4f}' for w in experiment['weights']]) + ']'
        content = re.sub(
            r'LAYER_AREA_WEIGHTS = \[.*?\]',
            f'LAYER_AREA_WEIGHTS = {weights_str}',
            content
        )

        with open(self.base_types, 'w', encoding='utf-8') as f:
            f.write(content)

    def _modify_main_script(self, experiment: Dict):
        """ä¿®æ”¹ä¸»ç¨‹åºæ–‡ä»¶"""
        with open(self.base_script, 'r', encoding='utf-8') as f:
            content = f.read()

        # ä¿®æ”¹ lv_grads
        lv_grads_str = '[' + ', '.join([f'{g:.3f}' for g in experiment['lv_grads']]) + ']'
        content = re.sub(
            r'config\.lv_grads = \[.*?\]',
            f'config.lv_grads = {lv_grads_str}',
            content
        )

        # ä¿®æ”¹ q_levels
        q_levels_str = str(experiment['q_levels'])
        content = re.sub(
            r'config\.q_levels = \[.*?\]',
            f'config.q_levels = {q_levels_str}',
            content
        )

        with open(self.base_script, 'w', encoding='utf-8') as f:
            f.write(content)

    def run_single_experiment(self, experiment: Dict) -> Dict:
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        experiment_name = experiment['name']
        layers_count = experiment['layers_count']
        layer_interval = experiment['layer_interval']

        print(f"\n{'=' * 80}")
        print(f"å¼€å§‹å®éªŒ: {experiment_name}")
        print(f"å±‚æ•°: {layers_count}, å±‚é—´éš”: {layer_interval:.3f}m")
        print(f"é«˜åº¦åˆ†å‰²: {experiment['lv_grads']}")
        print(f"{'=' * 80}")

        # ä¿®æ”¹é…ç½®æ–‡ä»¶
        self.modify_config_files(experiment)

        # === æ–°å¢ï¼šå¼ºåˆ¶é‡æ–°åŠ è½½æ¨¡å— ===
        import importlib
        import sys

        # å¦‚æœæ¨¡å—å·²ç»å¯¼å…¥ï¼Œé‡æ–°åŠ è½½å®ƒ
        if 'contour_manager_åŒºé—´åˆ†å‰²_å‚ç›´ç»“æ„å¤æ‚åº¦' in sys.modules:
            importlib.reload(sys.modules['contour_manager_åŒºé—´åˆ†å‰²_å‚ç›´ç»“æ„å¤æ‚åº¦'])
            print("[INFO] é‡æ–°åŠ è½½äº† contour_manager æ¨¡å—")

        if 'contour_database' in sys.modules:
            importlib.reload(sys.modules['contour_database'])
            print("[INFO] é‡æ–°åŠ è½½äº† contour_database æ¨¡å—")
        # === é‡æ–°åŠ è½½ç»“æŸ ===

        # è®¾ç½®æ—¥å¿—æ–‡ä»¶å
        log_file = f"comprehensive_{experiment_name}_log.txt"

        # è¿è¡Œå®éªŒ
        start_time = time.time()

        try:
            # è¿è¡Œä¸»ç¨‹åº
            result = subprocess.run([
                sys.executable, self.base_script,
                "--log_file", log_file
            ], capture_output=True, text=True, timeout=3600)  # 1å°æ—¶è¶…æ—¶

            if result.returncode != 0:
                print(f"å®éªŒ {experiment_name} è¿è¡Œå¤±è´¥:")
                print(f"STDOUT: {result.stdout}")
                print(f"STDERR: {result.stderr}")
                return None

        except subprocess.TimeoutExpired:
            print(f"å®éªŒ {experiment_name} è¿è¡Œè¶…æ—¶")
            return None
        except Exception as e:
            print(f"å®éªŒ {experiment_name} è¿è¡Œå¼‚å¸¸: {e}")
            return None

        end_time = time.time()
        duration = end_time - start_time

        # è§£æåŸºæœ¬ç»“æœ
        result_data = self._parse_basic_results(log_file, experiment, duration)

        if result_data:
            # è¿›è¡Œè¯¦ç»†çš„ç‰¹å¾åˆ†æ
            feature_analysis = self._perform_detailed_feature_analysis(log_file, experiment)
            result_data.update(feature_analysis)

            # åœ¨æ—¥å¿—æ–‡ä»¶æœ«å°¾æ·»åŠ åˆ†æä¿¡æ¯
            self._append_analysis_info_to_log(log_file, experiment, duration, feature_analysis)

        print(f"å®éªŒ {experiment_name} å®Œæˆ")
        print(f"è¿è¡Œæ—¶é•¿: {duration:.2f}ç§’")
        if result_data:
            print(f"Recall@1: {result_data['recall_1']:.2f}%")
            print(f"å¹³å‡è½®å»“æ•°: {result_data.get('avg_contours_per_frame', 0):.1f}")
            print(f"ç‰¹å¾è´¨é‡æŒ‡æ•°: {result_data.get('feature_quality_index', 0):.3f}")

        return result_data

    def _parse_basic_results(self, log_file: str, experiment: Dict, duration: float) -> Dict:
        """è§£æåŸºæœ¬å®éªŒç»“æœ"""
        if not os.path.exists(log_file):
            print(f"è­¦å‘Š: æ—¥å¿—æ–‡ä»¶ {log_file} ä¸å­˜åœ¨")
            return None

        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                content = f.read()

            result = {
                'name': experiment['name'],
                'layers_count': experiment['layers_count'],
                'layer_interval': experiment['layer_interval'],
                'duration': duration,
                'log_file': log_file,
                'recall_1': 0.0,
                'recall_5': 0.0,
                'recall_10': 0.0,
                'recall_25': 0.0,
                'similarity': 0.0,
                'top1_recall': 0.0
            }

            # æå–å¬å›ç‡
            recall_1_match = re.search(r'Average Recall @1: ([\d.]+)%', content)
            if recall_1_match:
                result['recall_1'] = float(recall_1_match.group(1))

            recall_5_match = re.search(r'Average Recall @5: ([\d.]+)%', content)
            if recall_5_match:
                result['recall_5'] = float(recall_5_match.group(1))

            recall_10_match = re.search(r'Average Recall @10: ([\d.]+)%', content)
            if recall_10_match:
                result['recall_10'] = float(recall_10_match.group(1))

            recall_25_match = re.search(r'Average Recall @25: ([\d.]+)%', content)
            if recall_25_match:
                result['recall_25'] = float(recall_25_match.group(1))

            # æå–ç›¸ä¼¼æ€§
            similarity_match = re.search(r'Average Similarity: ([\d.]+)', content)
            if similarity_match:
                result['similarity'] = float(similarity_match.group(1))

            # æå–Top 1% Recall
            top1_match = re.search(r'Average Top 1% Recall: ([\d.]+)%', content)
            if top1_match:
                result['top1_recall'] = float(top1_match.group(1))

            return result

        except Exception as e:
            print(f"è§£æåŸºæœ¬ç»“æœå¤±è´¥: {e}")
            return None

    def _perform_detailed_feature_analysis(self, log_file: str, experiment: Dict) -> Dict:
        """æ‰§è¡Œè¯¦ç»†çš„ç‰¹å¾åˆ†æ"""
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                content = f.read()

            analysis = {}

            # 1. è½®å»“ç»Ÿè®¡åˆ†æ
            analysis.update(self._analyze_contour_statistics(content))

            # 2. è½®å»“å°ºå¯¸åˆ†å¸ƒåˆ†æ
            analysis.update(self._analyze_contour_size_distribution(content))

            # 3. è½®å»“å‡ ä½•ç‰¹å¾åˆ†æ
            analysis.update(self._analyze_contour_geometry_features(content))

            # 4. æ£€ç´¢é”®ç‰¹å¾åˆ†æ
            analysis.update(self._analyze_retrieval_key_features(content))

            # 5. BCIç‰¹å¾åˆ†æ
            analysis.update(self._analyze_bci_features(content))

            # 6. ç›¸ä¼¼æ€§æ£€æŸ¥åˆ†æ
            analysis.update(self._analyze_similarity_checks(content))

            # 7. è®¡ç®—ç»¼åˆç‰¹å¾è´¨é‡æŒ‡æ•°
            analysis['feature_quality_index'] = self._calculate_feature_quality_index(analysis)

            return analysis

        except Exception as e:
            print(f"è¯¦ç»†ç‰¹å¾åˆ†æå¤±è´¥: {e}")
            return {}

    def _analyze_contour_statistics(self, content: str) -> Dict:
        """åˆ†æè½®å»“ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_frames': 0,
            'total_contours': 0,
            'avg_contours_per_frame': 0.0,
            'std_contours_per_frame': 0.0,
            'min_contours_per_frame': 0,
            'max_contours_per_frame': 0
        }

        try:
            # æå–æ¯å¸§è½®å»“æ•°é‡
            frame_contours = re.findall(r'\[DB \d+\] Frame \d+: æ€»è½®å»“æ•°: (\d+)', content)

            if frame_contours:
                contour_counts = [int(x) for x in frame_contours]

                stats['total_frames'] = len(contour_counts)
                stats['total_contours'] = sum(contour_counts)
                stats['avg_contours_per_frame'] = statistics.mean(contour_counts)
                stats['std_contours_per_frame'] = statistics.stdev(contour_counts) if len(contour_counts) > 1 else 0
                stats['min_contours_per_frame'] = min(contour_counts)
                stats['max_contours_per_frame'] = max(contour_counts)

        except Exception as e:
            print(f"è½®å»“ç»Ÿè®¡åˆ†æå¤±è´¥: {e}")

        return stats

    def _analyze_contour_size_distribution(self, content: str) -> Dict:
        """åˆ†æè½®å»“å°ºå¯¸åˆ†å¸ƒ"""
        size_dist = {}

        try:
            # åˆå§‹åŒ–å„å°ºå¯¸åŒºé—´çš„è®¡æ•°
            for min_size, max_size, label in self.contour_size_bins:
                size_dist[f'contours_{label}_count'] = 0
                size_dist[f'contours_{label}_ratio'] = 0.0

            # æå–è½®å»“å°ºå¯¸ä¿¡æ¯
            contour_sizes_match = re.search(r'Contour sizes: ([\d,]+)', content)
            if contour_sizes_match:
                sizes_str = contour_sizes_match.group(1)
                sizes = [int(x) for x in sizes_str.split(',') if x.strip()]

                total_contours = len(sizes)
                if total_contours > 0:
                    for min_size, max_size, label in self.contour_size_bins:
                        if max_size == float('inf'):
                            count = sum(1 for s in sizes if s >= min_size)
                        else:
                            count = sum(1 for s in sizes if min_size <= s <= max_size)

                        size_dist[f'contours_{label}_count'] = count
                        size_dist[f'contours_{label}_ratio'] = count / total_contours

        except Exception as e:
            print(f"è½®å»“å°ºå¯¸åˆ†å¸ƒåˆ†æå¤±è´¥: {e}")

        return size_dist

    def _analyze_contour_geometry_features(self, content: str) -> Dict:
        """åˆ†æè½®å»“å‡ ä½•ç‰¹å¾"""
        geometry = {
            'avg_eccentricity': 0.0,
            'std_eccentricity': 0.0,
            'significant_ecc_features_ratio': 0.0,
            'significant_com_features_ratio': 0.0,
            'avg_eigenvalue_ratio': 0.0,
            'avg_contour_height': 0.0
        }

        try:
            # æå–åå¿ƒç‡ä¿¡æ¯
            ecc_match = re.search(r'Eccentricities: ([\d.,]+)', content)
            if ecc_match:
                ecc_str = ecc_match.group(1)
                eccentricities = [float(x) for x in ecc_str.split(',') if x.strip()]

                if eccentricities:
                    geometry['avg_eccentricity'] = statistics.mean(eccentricities)
                    geometry['std_eccentricity'] = statistics.stdev(eccentricities) if len(eccentricities) > 1 else 0

                    # è®¡ç®—å„åå¿ƒç‡åŒºé—´çš„åˆ†å¸ƒ
                    for min_ecc, max_ecc, label in self.eccentricity_bins:
                        count = sum(1 for e in eccentricities if min_ecc <= e < max_ecc)
                        geometry[f'{label}_ratio'] = count / len(eccentricities)

            # æå–ç‰¹å¾å€¼æ¯”ä¾‹ä¿¡æ¯
            eigenratio_match = re.search(r'Eigenvalue ratios: ([\d.,]+)', content)
            if eigenratio_match:
                ratios_str = eigenratio_match.group(1)
                ratios = [float(x) for x in ratios_str.split(',') if x.strip()]

                if ratios:
                    geometry['avg_eigenvalue_ratio'] = statistics.mean(ratios)

            # æå–æ˜¾è‘—ç‰¹å¾æ¯”ä¾‹
            significant_features = re.search(r'Significant features: ecc=(\d+), com=(\d+), total=(\d+)', content)
            if significant_features:
                ecc_count, com_count, total_count = map(int, significant_features.groups())
                if total_count > 0:
                    geometry['significant_ecc_features_ratio'] = ecc_count / total_count
                    geometry['significant_com_features_ratio'] = com_count / total_count

            # æå–é«˜åº¦ä¿¡æ¯
            heights_match = re.search(r'Contour heights: ([\d.,]+)', content)
            if heights_match:
                heights_str = heights_match.group(1)
                heights = [float(x) for x in heights_str.split(',') if x.strip()]

                if heights:
                    geometry['avg_contour_height'] = statistics.mean(heights)

        except Exception as e:
            print(f"è½®å»“å‡ ä½•ç‰¹å¾åˆ†æå¤±è´¥: {e}")

        return geometry

    def _analyze_retrieval_key_features(self, content: str) -> Dict:
        """åˆ†ææ£€ç´¢é”®ç‰¹å¾"""
        key_features = {
            'avg_key_dimension_0': 0.0,
            'avg_key_dimension_1': 0.0,
            'avg_key_dimension_2': 0.0,
            'key_sparsity_ratio': 0.0,
            'key_distinctiveness': 0.0,
            'ring_feature_activation': 0.0
        }

        try:
            # æå–é”®çš„å„ç»´åº¦ç»Ÿè®¡
            key_dim0_match = re.search(r'Key dimension 0: avg=([\d.]+)', content)
            if key_dim0_match:
                key_features['avg_key_dimension_0'] = float(key_dim0_match.group(1))

            key_dim1_match = re.search(r'Key dimension 1: avg=([\d.]+)', content)
            if key_dim1_match:
                key_features['avg_key_dimension_1'] = float(key_dim1_match.group(1))

            key_dim2_match = re.search(r'Key dimension 2: avg=([\d.]+)', content)
            if key_dim2_match:
                key_features['avg_key_dimension_2'] = float(key_dim2_match.group(1))

            # æå–ç¨€ç–æ€§ä¿¡æ¯
            sparsity_match = re.search(r'Key sparsity: ([\d.]+)', content)
            if sparsity_match:
                key_features['key_sparsity_ratio'] = float(sparsity_match.group(1))

            # æå–ç¯å½¢ç‰¹å¾æ¿€æ´»ç‡
            ring_activation_match = re.search(r'Ring feature activation: ([\d.]+)', content)
            if ring_activation_match:
                key_features['ring_feature_activation'] = float(ring_activation_match.group(1))

        except Exception as e:
            print(f"æ£€ç´¢é”®ç‰¹å¾åˆ†æå¤±è´¥: {e}")

        return key_features

    def _analyze_bci_features(self, content: str) -> Dict:
        """åˆ†æBCIç‰¹å¾"""
        bci_features = {
            'avg_neighbors_per_bci': 0.0,
            'std_neighbors_per_bci': 0.0,
            'max_neighbors_per_bci': 0,
            'avg_neighbor_distance': 0.0,
            'std_neighbor_distance': 0.0,
            'avg_neighbor_angle_diversity': 0.0,
            'cross_layer_connections_ratio': 0.0,
            'distance_bit_activation_rate': 0.0,
            'constellation_complexity': 0.0
        }

        try:
            # æå–BCIé‚»å±…æ•°é‡ä¿¡æ¯
            neighbors_match = re.search(r'BCI neighbors: ([\d,]+)', content)
            if neighbors_match:
                neighbors_str = neighbors_match.group(1)
                neighbor_counts = [int(x) for x in neighbors_str.split(',') if x.strip()]

                if neighbor_counts:
                    bci_features['avg_neighbors_per_bci'] = statistics.mean(neighbor_counts)
                    bci_features['std_neighbors_per_bci'] = statistics.stdev(neighbor_counts) if len(
                        neighbor_counts) > 1 else 0
                    bci_features['max_neighbors_per_bci'] = max(neighbor_counts)

            # æå–é‚»å±…è·ç¦»ä¿¡æ¯
            distances_match = re.search(r'Neighbor distances: ([\d.,]+)', content)
            if distances_match:
                distances_str = distances_match.group(1)
                distances = [float(x) for x in distances_str.split(',') if x.strip()]

                if distances:
                    bci_features['avg_neighbor_distance'] = statistics.mean(distances)
                    bci_features['std_neighbor_distance'] = statistics.stdev(distances) if len(distances) > 1 else 0

            # æå–è§’åº¦ä¿¡æ¯
            angles_match = re.search(r'Neighbor angles: ([\d.,\-]+)', content)
            if angles_match:
                angles_str = angles_match.group(1)
                angles = [float(x) for x in angles_str.split(',') if x.strip()]

                if angles:
                    # è®¡ç®—è§’åº¦å¤šæ ·æ€§ï¼ˆæ ‡å‡†å·®ï¼‰
                    bci_features['avg_neighbor_angle_diversity'] = statistics.stdev(angles) if len(angles) > 1 else 0

            # æå–è·¨å±‚è¿æ¥ä¿¡æ¯
            cross_layer_match = re.search(r'Cross layer connections: (\d+)/(\d+)', content)
            if cross_layer_match:
                cross_layer_count = int(cross_layer_match.group(1))
                total_connections = int(cross_layer_match.group(2))
                if total_connections > 0:
                    bci_features['cross_layer_connections_ratio'] = cross_layer_count / total_connections

            # æå–è·ç¦»ä½æ¿€æ´»ç‡
            bit_activation_match = re.search(r'Distance bit activation: ([\d.]+)', content)
            if bit_activation_match:
                bci_features['distance_bit_activation_rate'] = float(bit_activation_match.group(1))

            # è®¡ç®—æ˜Ÿåº§å¤æ‚åº¦ï¼ˆåŸºäºå¹³å‡é‚»å±…æ•°å’Œè§’åº¦å¤šæ ·æ€§ï¼‰
            avg_neighbors = bci_features['avg_neighbors_per_bci']
            angle_diversity = bci_features['avg_neighbor_angle_diversity']
            if avg_neighbors > 0 and angle_diversity > 0:
                bci_features['constellation_complexity'] = avg_neighbors * angle_diversity / 10.0  # å½’ä¸€åŒ–

        except Exception as e:
            print(f"BCIç‰¹å¾åˆ†æå¤±è´¥: {e}")

        return bci_features

    def _analyze_similarity_checks(self, content: str) -> Dict:
        """åˆ†æç›¸ä¼¼æ€§æ£€æŸ¥ç»Ÿè®¡"""
        similarity_stats = {
            'contour_similarity_pass_rate': 0.0,
            'constellation_similarity_pass_rate': 0.0,
            'pairwise_similarity_pass_rate': 0.0,
            'area_check_fail_rate': 0.0,
            'eigenvalue_check_fail_rate': 0.0,
            'height_check_fail_rate': 0.0,
            'centroid_check_fail_rate': 0.0,
            'geometric_consistency_pass_rate': 0.0
        }

        try:
            # æå–å„æ£€æŸ¥æ­¥éª¤çš„é€šè¿‡ç‡
            check1_match = re.search(r'After check 1: (\d+)', content)
            check2_match = re.search(r'After check 2: (\d+)', content)
            check3_match = re.search(r'After check 3: (\d+)', content)

            if check1_match and check2_match and check3_match:
                check1_count = int(check1_match.group(1))
                check2_count = int(check2_match.group(1))
                check3_count = int(check3_match.group(1))

                # å‡è®¾æ€»æŸ¥è¯¢æ•°ä¸º1000ï¼ˆå¯ä»¥æ ¹æ®å®é™…è°ƒæ•´ï¼‰
                total_queries = 1000
                if total_queries > 0:
                    similarity_stats['contour_similarity_pass_rate'] = check1_count / total_queries

                if check1_count > 0:
                    similarity_stats['constellation_similarity_pass_rate'] = check2_count / check1_count

                if check2_count > 0:
                    similarity_stats['pairwise_similarity_pass_rate'] = check3_count / check2_count

            # æå–è¯¦ç»†çš„ç›¸ä¼¼æ€§æ£€æŸ¥é€šè¿‡ç‡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            sim_pass_rate_match = re.search(r'Similarity check pass rates: check1=([\d.]+)', content)
            if sim_pass_rate_match:
                similarity_stats['contour_similarity_pass_rate'] = float(sim_pass_rate_match.group(1))

            const_pass_rate_match = re.search(r'Constellation similarity pass rate: ([\d.]+)', content)
            if const_pass_rate_match:
                similarity_stats['constellation_similarity_pass_rate'] = float(const_pass_rate_match.group(1))

            pair_pass_rate_match = re.search(r'Pairwise similarity pass rate: ([\d.]+)', content)
            if pair_pass_rate_match:
                similarity_stats['pairwise_similarity_pass_rate'] = float(pair_pass_rate_match.group(1))

        except Exception as e:
            print(f"ç›¸ä¼¼æ€§æ£€æŸ¥åˆ†æå¤±è´¥: {e}")

        return similarity_stats

    def _calculate_feature_quality_index(self, analysis: Dict) -> float:
        """è®¡ç®—ç»¼åˆç‰¹å¾è´¨é‡æŒ‡æ•°"""
        try:
            # ç»¼åˆå¤šä¸ªæŒ‡æ ‡è®¡ç®—ç‰¹å¾è´¨é‡
            quality_factors = []

            # è½®å»“è´¨é‡å› å­ï¼ˆè½®å»“æ•°é‡é€‚ä¸­ï¼Œä¸è¿‡åº¦ç¢ç‰‡åŒ–ï¼‰
            avg_contours = analysis.get('avg_contours_per_frame', 0)
            if avg_contours > 0:
                contour_quality = min(1.0, avg_contours / 200.0)  # 200ä¸ºç†æƒ³è½®å»“æ•°
                # æƒ©ç½šè¿‡å¤šçš„å°è½®å»“
                small_contour_penalty = analysis.get('contours_å°è½®å»“_ratio', 0) * 0.5
                contour_quality = max(0.1, contour_quality - small_contour_penalty)
                quality_factors.append(contour_quality)

            # å‡ ä½•ç‰¹å¾è´¨é‡å› å­ï¼ˆåå¿ƒç‡å’Œç‰¹å¾å€¼æ¯”ä¾‹çš„åˆç†æ€§ï¼‰
            avg_ecc = analysis.get('avg_eccentricity', 0)
            if avg_ecc > 0:
                # é€‚åº¦çš„åå¿ƒç‡è¡¨ç¤ºè‰¯å¥½çš„ç»“æ„ç‰¹å¾
                ecc_quality = 1.0 - abs(avg_ecc - 0.5) * 2  # 0.5ä¸ºç†æƒ³åå¿ƒç‡
                ecc_quality = max(0.1, ecc_quality)
                quality_factors.append(ecc_quality)

            # æ£€ç´¢é”®è´¨é‡å› å­ï¼ˆéç¨€ç–ä¸”æœ‰åŒºåˆ†åº¦ï¼‰
            key_sparsity = analysis.get('key_sparsity_ratio', 1.0)
            if key_sparsity < 1.0:
                key_quality = 1.0 - key_sparsity
                quality_factors.append(key_quality)

            # BCIè¿æ¥è´¨é‡å› å­ï¼ˆé€‚åº¦çš„é‚»å±…æ•°é‡ï¼‰
            avg_neighbors = analysis.get('avg_neighbors_per_bci', 0)
            if avg_neighbors > 0:
                # 3-8ä¸ªé‚»å±…ä¸ºç†æƒ³èŒƒå›´
                if 3 <= avg_neighbors <= 8:
                    bci_quality = 1.0
                elif avg_neighbors < 3:
                    bci_quality = avg_neighbors / 3.0
                else:
                    bci_quality = max(0.1, 1.0 - (avg_neighbors - 8) * 0.1)
                quality_factors.append(bci_quality)

            # ç›¸ä¼¼æ€§æ£€æŸ¥è´¨é‡å› å­ï¼ˆåˆç†çš„é€šè¿‡ç‡ï¼‰
            check_pass_rate = analysis.get('constellation_similarity_pass_rate', 0)
            if check_pass_rate > 0:
                # 10-50%çš„é€šè¿‡ç‡ä¸ºåˆç†èŒƒå›´
                if 0.1 <= check_pass_rate <= 0.5:
                    check_quality = 1.0
                elif check_pass_rate < 0.1:
                    check_quality = check_pass_rate / 0.1
                else:
                    check_quality = max(0.1, 1.0 - (check_pass_rate - 0.5) * 2)
                quality_factors.append(check_quality)

            # è®¡ç®—ç»¼åˆè´¨é‡æŒ‡æ•°
            if quality_factors:
                feature_quality_index = statistics.mean(quality_factors)
            else:
                feature_quality_index = 0.0

            return round(feature_quality_index, 4)

        except Exception as e:
            print(f"è®¡ç®—ç‰¹å¾è´¨é‡æŒ‡æ•°å¤±è´¥: {e}")
            return 0.0

    def _append_analysis_info_to_log(self, log_file: str, experiment: Dict,
                                     duration: float, analysis: Dict):
        """åœ¨æ—¥å¿—æ–‡ä»¶æœ«å°¾æ·»åŠ åˆ†æä¿¡æ¯"""
        try:
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(f"\n{'=' * 80}\n")
                f.write(f"ç»¼åˆç‰¹å¾åˆ†ææŠ¥å‘Š\n")
                f.write(f"{'=' * 80}\n")
                f.write(f"å®éªŒé…ç½®: {experiment['name']} ({experiment['layers_count']}å±‚)\n")
                f.write(f"å±‚é—´éš”: {experiment['layer_interval']:.3f}m\n")
                f.write(f"è¿è¡Œæ—¶é•¿: {duration:.2f}ç§’\n")
                f.write(f"ç‰¹å¾è´¨é‡æŒ‡æ•°: {analysis.get('feature_quality_index', 0):.4f}\n")
                f.write(f"å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")

                # è½®å»“ç»Ÿè®¡
                f.write("è½®å»“ç»Ÿè®¡åˆ†æ:\n")
                f.write("-" * 40 + "\n")
                f.write(f"æ€»å¸§æ•°: {analysis.get('total_frames', 0)}\n")
                f.write(f"æ€»è½®å»“æ•°: {analysis.get('total_contours', 0)}\n")
                f.write(f"å¹³å‡æ¯å¸§è½®å»“æ•°: {analysis.get('avg_contours_per_frame', 0):.1f}\n")
                f.write(f"è½®å»“æ•°æ ‡å‡†å·®: {analysis.get('std_contours_per_frame', 0):.1f}\n")
                f.write(f"æœ€å°‘è½®å»“æ•°: {analysis.get('min_contours_per_frame', 0)}\n")
                f.write(f"æœ€å¤šè½®å»“æ•°: {analysis.get('max_contours_per_frame', 0)}\n\n")

                # è½®å»“å°ºå¯¸åˆ†å¸ƒ
                f.write("è½®å»“å°ºå¯¸åˆ†å¸ƒ:\n")
                f.write("-" * 40 + "\n")
                for min_size, max_size, label in self.contour_size_bins:
                    count = analysis.get(f'contours_{label}_count', 0)
                    ratio = analysis.get(f'contours_{label}_ratio', 0)
                    if max_size == float('inf'):
                        size_range = f"â‰¥{min_size}åƒç´ "
                    else:
                        size_range = f"{min_size}-{max_size}åƒç´ "
                    f.write(f"{label} ({size_range}): {count}ä¸ª ({ratio:.1%})\n")
                f.write("\n")

                # å‡ ä½•ç‰¹å¾
                f.write("è½®å»“å‡ ä½•ç‰¹å¾:\n")
                f.write("-" * 40 + "\n")
                f.write(f"å¹³å‡åå¿ƒç‡: {analysis.get('avg_eccentricity', 0):.3f}\n")
                f.write(f"åå¿ƒç‡æ ‡å‡†å·®: {analysis.get('std_eccentricity', 0):.3f}\n")
                f.write(f"æ˜¾è‘—åå¿ƒç‡ç‰¹å¾æ¯”ä¾‹: {analysis.get('significant_ecc_features_ratio', 0):.1%}\n")
                f.write(f"æ˜¾è‘—è´¨å¿ƒç‰¹å¾æ¯”ä¾‹: {analysis.get('significant_com_features_ratio', 0):.1%}\n")
                f.write(f"å¹³å‡ç‰¹å¾å€¼æ¯”ä¾‹: {analysis.get('avg_eigenvalue_ratio', 0):.3f}\n")
                for min_ecc, max_ecc, label in self.eccentricity_bins:
                    ratio = analysis.get(f'{label}_ratio', 0)
                    f.write(f"{label} ({min_ecc:.1f}-{max_ecc:.1f}): {ratio:.1%}\n")
                f.write("\n")

                # æ£€ç´¢é”®ç‰¹å¾
                f.write("æ£€ç´¢é”®ç‰¹å¾:\n")
                f.write("-" * 40 + "\n")
                f.write(f"ä¸»ç‰¹å¾å€¼*é¢ç§¯ å¹³å‡å€¼: {analysis.get('avg_key_dimension_0', 0):.2f}\n")
                f.write(f"æ¬¡ç‰¹å¾å€¼*é¢ç§¯ å¹³å‡å€¼: {analysis.get('avg_key_dimension_1', 0):.2f}\n")
                f.write(f"ç´¯ç§¯é¢ç§¯ å¹³å‡å€¼: {analysis.get('avg_key_dimension_2', 0):.2f}\n")
                f.write(f"é”®ç¨€ç–æ€§æ¯”ä¾‹: {analysis.get('key_sparsity_ratio', 0):.1%}\n")
                f.write(f"é”®åŒºåˆ†åº¦: {analysis.get('key_distinctiveness', 0):.3f}\n")
                f.write(f"ç¯å½¢ç‰¹å¾æ¿€æ´»ç‡: {analysis.get('ring_feature_activation', 0):.1%}\n\n")

                # BCIç‰¹å¾
                f.write("BCIç‰¹å¾:\n")
                f.write("-" * 40 + "\n")
                f.write(f"å¹³å‡é‚»å±…æ•°: {analysis.get('avg_neighbors_per_bci', 0):.1f}\n")
                f.write(f"é‚»å±…æ•°æ ‡å‡†å·®: {analysis.get('std_neighbors_per_bci', 0):.1f}\n")
                f.write(f"æœ€å¤§é‚»å±…æ•°: {analysis.get('max_neighbors_per_bci', 0)}\n")
                f.write(f"å¹³å‡é‚»å±…è·ç¦»: {analysis.get('avg_neighbor_distance', 0):.2f}\n")
                f.write(f"é‚»å±…è·ç¦»æ ‡å‡†å·®: {analysis.get('std_neighbor_distance', 0):.2f}\n")
                f.write(f"è·¨å±‚è¿æ¥æ¯”ä¾‹: {analysis.get('cross_layer_connections_ratio', 0):.1%}\n")
                f.write(f"è·ç¦»ä½æ¿€æ´»ç‡: {analysis.get('distance_bit_activation_rate', 0):.1%}\n")
                f.write(f"æ˜Ÿåº§å¤æ‚åº¦: {analysis.get('constellation_complexity', 0):.3f}\n\n")

                # ç›¸ä¼¼æ€§æ£€æŸ¥
                f.write("ç›¸ä¼¼æ€§æ£€æŸ¥ç»Ÿè®¡:\n")
                f.write("-" * 40 + "\n")
                f.write(f"è½®å»“ç›¸ä¼¼æ€§é€šè¿‡ç‡: {analysis.get('contour_similarity_pass_rate', 0):.1%}\n")
                f.write(f"æ˜Ÿåº§ç›¸ä¼¼æ€§é€šè¿‡ç‡: {analysis.get('constellation_similarity_pass_rate', 0):.1%}\n")
                f.write(f"æˆå¯¹ç›¸ä¼¼æ€§é€šè¿‡ç‡: {analysis.get('pairwise_similarity_pass_rate', 0):.1%}\n")
                f.write(f"é¢ç§¯æ£€æŸ¥å¤±è´¥ç‡: {analysis.get('area_check_fail_rate', 0):.1%}\n")
                f.write(f"ç‰¹å¾å€¼æ£€æŸ¥å¤±è´¥ç‡: {analysis.get('eigenvalue_check_fail_rate', 0):.1%}\n")
                f.write(f"é«˜åº¦æ£€æŸ¥å¤±è´¥ç‡: {analysis.get('height_check_fail_rate', 0):.1%}\n")
                f.write(f"è´¨å¿ƒæ£€æŸ¥å¤±è´¥ç‡: {analysis.get('centroid_check_fail_rate', 0):.1%}\n")

                f.write(f"{'=' * 80}\n")

        except Exception as e:
            print(f"æ·»åŠ åˆ†æä¿¡æ¯å¤±è´¥: {e}")

    def run_all_experiments(self):
        """è¿è¡Œæ‰€æœ‰å®éªŒ"""
        print("å¼€å§‹ç»¼åˆåˆ†å±‚æ•°é‡åˆ†æå®éªŒ")
        print(f"å®éªŒèŒƒå›´: {len(self.experiments)} ä¸ªé…ç½® (1-20å±‚)")
        print(f"é¢„è®¡æ€»æ—¶é•¿: {len(self.experiments) * 15} åˆ†é’Ÿ")

        # å¤‡ä»½åŸå§‹æ–‡ä»¶
        self.backup_files()

        try:
            # è¿è¡Œæ‰€æœ‰å®éªŒ
            for i, experiment in enumerate(self.experiments, 1):
                print(f"\næ€»è¿›åº¦: {i}/{len(self.experiments)}")

                result = self.run_single_experiment(experiment)
                if result:
                    self.results.append(result)
                else:
                    print(f"å®éªŒ {experiment['name']} å¤±è´¥ï¼Œè·³è¿‡")

            # ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
            self.generate_comprehensive_report()

        finally:
            # æ¢å¤åŸå§‹æ–‡ä»¶
            self.restore_files()

    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        if not self.results:
            print("æ²¡æœ‰æˆåŠŸçš„å®éªŒç»“æœ")
            return

        # åˆ›å»ºç»¼åˆæŠ¥å‘Šæ–‡ä»¶
        report_file = "comprehensive_layers_analysis_report.txt"

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("ç»¼åˆåˆ†å±‚æ•°é‡åˆ†æå®éªŒæŠ¥å‘Š\n")
            f.write("=" * 100 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"å®éªŒæ•°é‡: {len(self.results)}\n")
            f.write(f"å±‚æ•°èŒƒå›´: 1-20å±‚\n")
            f.write(f"åˆ†æç›®çš„: æ·±å…¥ç†è§£å±‚æ•°å˜åŒ–å¯¹ç‰¹å¾æå–å’Œå¬å›ç‡çš„å½±å“\n\n")

            # åŸºæœ¬æ€§èƒ½è¡¨æ ¼
            f.write("åŸºæœ¬æ€§èƒ½ç»“æœ:\n")
            f.write("-" * 120 + "\n")
            header = (f"{'å±‚æ•°':<4} {'å±‚é—´éš”':<8} {'Recall@1':<10} {'ç›¸ä¼¼æ€§':<8} {'è¿è¡Œæ—¶é•¿':<8} "
                      f"{'å¹³å‡è½®å»“':<8} {'ç‰¹å¾è´¨é‡':<8} {'å°è½®å»“æ¯”ä¾‹':<10} {'BCIé‚»å±…':<8}\n")
            f.write(header)
            f.write("-" * 120 + "\n")

            # æŒ‰å±‚æ•°æ’åº
            sorted_results = sorted(self.results, key=lambda x: x['layers_count'])

            for result in sorted_results:
                line = (f"{result['layers_count']:<4d} {result['layer_interval']:<8.3f} "
                        f"{result['recall_1']:<10.2f} {result['similarity']:<8.4f} "
                        f"{result['duration']:<8.0f} {result.get('avg_contours_per_frame', 0):<8.1f} "
                        f"{result.get('feature_quality_index', 0):<8.3f} "
                        f"{result.get('contours_å°è½®å»“_ratio', 0):<10.2f} "
                        f"{result.get('avg_neighbors_per_bci', 0):<8.1f}\n")
                f.write(line)

            f.write("-" * 120 + "\n\n")

            # æ€§èƒ½è¶‹åŠ¿åˆ†æ
            f.write("æ€§èƒ½è¶‹åŠ¿åˆ†æ:\n")
            f.write("-" * 50 + "\n")

            recall_values = [r['recall_1'] for r in sorted_results]
            if recall_values:
                max_recall = max(recall_values)
                max_idx = recall_values.index(max_recall)
                optimal_layers = sorted_results[max_idx]['layers_count']

                f.write(f"æœ€é«˜å¬å›ç‡: {max_recall:.2f}% (åœ¨{optimal_layers}å±‚æ—¶è¾¾åˆ°)\n")
                f.write(f"æœ€ä¼˜å±‚é—´éš”: {sorted_results[max_idx]['layer_interval']:.3f}m\n")

                # å¯»æ‰¾æ€§èƒ½ä¸‹é™ç‚¹
                if len(recall_values) >= optimal_layers:
                    decline_threshold = max_recall * 0.95  # 95%é˜ˆå€¼
                    decline_start = -1

                    for i in range(max_idx, len(recall_values)):
                        if recall_values[i] < decline_threshold:
                            decline_start = i
                            break

                    if decline_start != -1:
                        f.write(f"æ€§èƒ½å¼€å§‹ä¸‹é™: ä»{sorted_results[decline_start]['layers_count']}å±‚å¼€å§‹\n")
                        f.write(f"è¿‡åˆ†å‰²é˜ˆå€¼: çº¦{sorted_results[decline_start]['layer_interval']:.3f}må±‚é—´éš”\n")
                    else:
                        f.write("åœ¨æµ‹è¯•èŒƒå›´å†…æœªè§‚å¯Ÿåˆ°æ˜æ˜¾æ€§èƒ½ä¸‹é™\n")
                        f.write("å»ºè®®æ‰©å±•åˆ°æ›´å¤šå±‚æ•°ä»¥æ‰¾åˆ°è¿‡åˆ†å‰²é˜ˆå€¼\n")

            f.write("\n")

            # ç‰¹å¾è´¨é‡åˆ†æ
            f.write("ç‰¹å¾è´¨é‡åˆ†æ:\n")
            f.write("-" * 50 + "\n")

            quality_values = [r.get('feature_quality_index', 0) for r in sorted_results]
            if quality_values:
                avg_quality = statistics.mean(quality_values)
                f.write(f"å¹³å‡ç‰¹å¾è´¨é‡æŒ‡æ•°: {avg_quality:.3f}\n")

                # æ‰¾å‡ºç‰¹å¾è´¨é‡æœ€é«˜çš„é…ç½®
                max_quality = max(quality_values)
                max_quality_idx = quality_values.index(max_quality)
                f.write(f"æœ€é«˜ç‰¹å¾è´¨é‡: {max_quality:.3f} (åœ¨{sorted_results[max_quality_idx]['layers_count']}å±‚æ—¶)\n")

            # è½®å»“ç¢ç‰‡åŒ–åˆ†æ
            f.write("\nè½®å»“ç¢ç‰‡åŒ–åˆ†æ:\n")
            f.write("-" * 50 + "\n")

            small_contour_ratios = [r.get('contours_å°è½®å»“_ratio', 0) for r in sorted_results]
            avg_contour_counts = [r.get('avg_contours_per_frame', 0) for r in sorted_results]

            if small_contour_ratios and avg_contour_counts:
                # åˆ†æè½®å»“ç¢ç‰‡åŒ–è¶‹åŠ¿
                f.write("å±‚æ•° -> å°è½®å»“æ¯”ä¾‹ -> å¹³å‡è½®å»“æ•° çš„å…³ç³»:\n")
                for i, result in enumerate(sorted_results):
                    if i % 3 == 0:  # æ¯3å±‚æ˜¾ç¤ºä¸€æ¬¡
                        f.write(f"{result['layers_count']:2d}å±‚: "
                                f"å°è½®å»“æ¯”ä¾‹={small_contour_ratios[i]:.1%}, "
                                f"å¹³å‡è½®å»“æ•°={avg_contour_counts[i]:.1f}\n")

                # è®¡ç®—ç¢ç‰‡åŒ–è¶‹åŠ¿
                if len(small_contour_ratios) > 5:
                    early_avg = statistics.mean(small_contour_ratios[:5])
                    late_avg = statistics.mean(small_contour_ratios[-5:])
                    f.write(f"\nç¢ç‰‡åŒ–è¶‹åŠ¿: å‰5å±‚å¹³å‡{early_avg:.1%} -> å5å±‚å¹³å‡{late_avg:.1%}\n")
                    if late_avg > early_avg * 1.2:
                        f.write("ç»“è®º: å­˜åœ¨æ˜æ˜¾çš„è½®å»“ç¢ç‰‡åŒ–è¶‹åŠ¿\n")
                    else:
                        f.write("ç»“è®º: è½®å»“ç¢ç‰‡åŒ–ä¸æ˜æ˜¾\n")

            # BCIå¤æ‚åº¦åˆ†æ
            f.write("\nBCIå¤æ‚åº¦åˆ†æ:\n")
            f.write("-" * 50 + "\n")

            neighbor_counts = [r.get('avg_neighbors_per_bci', 0) for r in sorted_results]
            if neighbor_counts:
                f.write("BCIé‚»å±…æ•°éšå±‚æ•°å˜åŒ–:\n")
                for i, result in enumerate(sorted_results):
                    if i % 4 == 0:  # æ¯4å±‚æ˜¾ç¤ºä¸€æ¬¡
                        f.write(f"{result['layers_count']:2d}å±‚: å¹³å‡é‚»å±…æ•°={neighbor_counts[i]:.1f}\n")

                # åˆ†æå¤æ‚åº¦è¶‹åŠ¿
                if len(neighbor_counts) > 5:
                    early_neighbors = statistics.mean(neighbor_counts[:5])
                    late_neighbors = statistics.mean(neighbor_counts[-5:])
                    f.write(f"\nå¤æ‚åº¦å˜åŒ–: å‰5å±‚å¹³å‡{early_neighbors:.1f}ä¸ªé‚»å±… -> "
                            f"å5å±‚å¹³å‡{late_neighbors:.1f}ä¸ªé‚»å±…\n")

            # å› æœå…³ç³»åˆ†æ
            f.write("\nå› æœå…³ç³»åˆ†æ:\n")
            f.write("=" * 60 + "\n")

            # å»ºç«‹ç‰¹å¾æŒ‡æ ‡ä¸æ€§èƒ½çš„ç›¸å…³æ€§
            if len(sorted_results) >= 10:
                f.write("ç‰¹å¾æŒ‡æ ‡ä¸å¬å›ç‡çš„å…³ç³»åˆ†æ:\n\n")

                # å±‚é—´éš” vs å¬å›ç‡
                intervals = [r['layer_interval'] for r in sorted_results]
                recalls = [r['recall_1'] for r in sorted_results]

                f.write("1. å±‚é—´éš”ä¸å¬å›ç‡å…³ç³»:\n")
                f.write("   æœ€ä¼˜å±‚é—´éš”åŒºé—´: ")

                # æ‰¾å‡ºå¬å›ç‡å‰25%çš„å±‚é—´éš”èŒƒå›´
                top_25_percent_count = max(1, len(recalls) // 4)
                top_indices = sorted(range(len(recalls)), key=lambda i: recalls[i], reverse=True)[:top_25_percent_count]
                top_intervals = [intervals[i] for i in top_indices]

                if top_intervals:
                    f.write(f"{min(top_intervals):.3f}m - {max(top_intervals):.3f}m\n")
                    f.write(f"   å¯¹åº”å±‚æ•°èŒƒå›´: {min([sorted_results[i]['layers_count'] for i in top_indices])}-"
                            f"{max([sorted_results[i]['layers_count'] for i in top_indices])}å±‚\n")

                # è½®å»“è´¨é‡ vs å¬å›ç‡
                f.write("\n2. è½®å»“ç¢ç‰‡åŒ–å¯¹æ€§èƒ½çš„å½±å“:\n")
                avg_contours = [r.get('avg_contours_per_frame', 0) for r in sorted_results]

                # æ‰¾å‡ºè½®å»“æ•°é€‚ä¸­ä¸”å¬å›ç‡é«˜çš„é…ç½®
                moderate_contour_configs = []
                for i, result in enumerate(sorted_results):
                    contour_count = avg_contours[i]
                    if 50 <= contour_count <= 200:  # é€‚ä¸­çš„è½®å»“æ•°é‡
                        moderate_contour_configs.append((result['layers_count'], recalls[i], contour_count))

                if moderate_contour_configs:
                    best_moderate = max(moderate_contour_configs, key=lambda x: x[1])
                    f.write(f"   è½®å»“æ•°é€‚ä¸­æ—¶çš„æœ€ä½³æ€§èƒ½: {best_moderate[0]}å±‚ "
                            f"({best_moderate[2]:.1f}ä¸ªè½®å»“, {best_moderate[1]:.1f}%å¬å›ç‡)\n")

                # BCIå¤æ‚åº¦ vs å¬å›ç‡
                f.write("\n3. BCIå¤æ‚åº¦å¯¹æ€§èƒ½çš„å½±å“:\n")
                bci_neighbors = [r.get('avg_neighbors_per_bci', 0) for r in sorted_results]

                # åˆ†æç†æƒ³çš„é‚»å±…æ•°é‡èŒƒå›´
                ideal_neighbor_configs = []
                for i, result in enumerate(sorted_results):
                    neighbor_count = bci_neighbors[i]
                    if 3 <= neighbor_count <= 8:  # ç†æƒ³çš„é‚»å±…æ•°é‡
                        ideal_neighbor_configs.append((result['layers_count'], recalls[i], neighbor_count))

                if ideal_neighbor_configs:
                    best_ideal = max(ideal_neighbor_configs, key=lambda x: x[1])
                    f.write(f"   BCIé‚»å±…æ•°é€‚ä¸­æ—¶çš„æœ€ä½³æ€§èƒ½: {best_ideal[0]}å±‚ "
                            f"({best_ideal[2]:.1f}ä¸ªé‚»å±…, {best_ideal[1]:.1f}%å¬å›ç‡)\n")

            # æ¨èå»ºè®®
            f.write("\næ¨èå»ºè®®:\n")
            f.write("=" * 40 + "\n")

            if sorted_results:
                # åŸºäºç»¼åˆåˆ†æçš„æ¨è
                best_overall = max(sorted_results, key=lambda x: x['recall_1'])
                best_quality = max(sorted_results, key=lambda x: x.get('feature_quality_index', 0))
                best_efficiency = max(sorted_results, key=lambda x: x['recall_1'] / x['duration'])

                f.write(f"1. æœ€ä½³æ€§èƒ½é…ç½®: {best_overall['layers_count']}å±‚ ")
                f.write(f"(å¬å›ç‡: {best_overall['recall_1']:.1f}%)\n")

                f.write(f"2. æœ€ä½³ç‰¹å¾è´¨é‡é…ç½®: {best_quality['layers_count']}å±‚ ")
                f.write(f"(è´¨é‡æŒ‡æ•°: {best_quality.get('feature_quality_index', 0):.3f})\n")

                f.write(f"3. æœ€é«˜æ•ˆç‡é…ç½®: {best_efficiency['layers_count']}å±‚ ")
                f.write(f"(æ•ˆç‡æ¯”: {best_efficiency['recall_1'] / best_efficiency['duration']:.4f})\n")

                # å®é™…åº”ç”¨å»ºè®®
                f.write("\nå®é™…åº”ç”¨å»ºè®®:\n")

                if len(sorted_results) >= 15:
                    # å¦‚æœæµ‹è¯•äº†è¶³å¤Ÿå¤šçš„å±‚æ•°
                    high_perf_configs = [r for r in sorted_results if r['recall_1'] >= max(recall_values) * 0.95]
                    if high_perf_configs:
                        recommended = min(high_perf_configs, key=lambda x: x['duration'])
                        f.write(f"- æ¨èé…ç½®: {recommended['layers_count']}å±‚ ")
                        f.write(f"(å¹³è¡¡æ€§èƒ½{recommended['recall_1']:.1f}%å’Œæ•ˆç‡{recommended['duration']:.0f}s)\n")
                else:
                    f.write("- å»ºè®®æ‰©å±•æµ‹è¯•èŒƒå›´åˆ°æ›´å¤šå±‚æ•°ä»¥è·å¾—æ›´å…¨é¢çš„åˆ†æ\n")

        print(f"\nç»¼åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

        # æ§åˆ¶å°è¾“å‡ºå…³é”®å‘ç°
        self._print_key_findings()

    def _print_key_findings(self):
        """è¾“å‡ºå…³é”®å‘ç°"""
        print("\n" + "=" * 80)
        print("å…³é”®å‘ç°æ€»ç»“")
        print("=" * 80)

        if not self.results:
            return

        sorted_results = sorted(self.results, key=lambda x: x['layers_count'])
        recall_values = [r['recall_1'] for r in sorted_results]

        # æ‰¾å‡ºæœ€ä½³é…ç½®
        if recall_values:
            max_recall = max(recall_values)
            max_idx = recall_values.index(max_recall)
            optimal_config = sorted_results[max_idx]

            print(f"ğŸ¯ æœ€ä½³æ€§èƒ½: {optimal_config['layers_count']}å±‚")
            print(f"   å¬å›ç‡: {max_recall:.2f}%")
            print(f"   å±‚é—´éš”: {optimal_config['layer_interval']:.3f}m")
            print(f"   ç‰¹å¾è´¨é‡: {optimal_config.get('feature_quality_index', 0):.3f}")

        # åˆ†ææ€§èƒ½è¶‹åŠ¿
        if len(recall_values) >= 10:
            # å¯»æ‰¾æ€§èƒ½å³°å€¼å’Œä¸‹é™ç‚¹
            peak_found = False
            decline_found = False

            for i in range(1, len(recall_values) - 1):
                # å¯»æ‰¾å±€éƒ¨å³°å€¼
                if not peak_found and recall_values[i] > recall_values[i - 1] and recall_values[i] > recall_values[
                    i + 1]:
                    peak_layers = sorted_results[i]['layers_count']
                    peak_recall = recall_values[i]
                    print(f"ğŸ“ˆ æ€§èƒ½å³°å€¼: {peak_layers}å±‚ ({peak_recall:.2f}%)")
                    peak_found = True

            # å¯»æ‰¾æ˜æ˜¾ä¸‹é™ç‚¹
            threshold = max_recall * 0.9  # 90%é˜ˆå€¼
            for i in range(max_idx + 1, len(recall_values)):
                if recall_values[i] < threshold:
                    decline_layers = sorted_results[i]['layers_count']
                    decline_recall = recall_values[i]
                    print(f"ğŸ“‰ æ€§èƒ½ä¸‹é™ç‚¹: {decline_layers}å±‚ ({decline_recall:.2f}%)")
                    print(f"   è¿‡åˆ†å‰²é˜ˆå€¼: çº¦{sorted_results[i]['layer_interval']:.3f}m")
                    decline_found = True
                    break

            if not decline_found:
                print("âš ï¸  åœ¨æµ‹è¯•èŒƒå›´å†…æœªå‘ç°æ˜æ˜¾çš„è¿‡åˆ†å‰²ç°è±¡")
                print("   å»ºè®®æ‰©å±•åˆ°æ›´å¤šå±‚æ•° (å¦‚25-30å±‚) æ¥æ‰¾åˆ°æ€§èƒ½ä¸‹é™ç‚¹")

        # è½®å»“ç¢ç‰‡åŒ–åˆ†æ
        small_ratios = [r.get('contours_å°è½®å»“_ratio', 0) for r in sorted_results]
        if small_ratios:
            if len(small_ratios) >= 5:
                early_frag = statistics.mean(small_ratios[:5])
                late_frag = statistics.mean(small_ratios[-5:])

                if late_frag > early_frag * 1.5:
                    print(f"ğŸ” è½®å»“ç¢ç‰‡åŒ–è¶‹åŠ¿: {early_frag:.1%} -> {late_frag:.1%}")
                    print("   è¯å®äº†è¿‡åˆ†å‰²å¯¼è‡´è½®å»“ç¢ç‰‡åŒ–çš„å‡è®¾")

        print(f"\nè¯¦ç»†æŠ¥å‘Š: comprehensive_layers_analysis_report.txt")


def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹ç»¼åˆåˆ†å±‚æ•°é‡åˆ†æå®éªŒ")
    print("åˆ†æç›®æ ‡: æ·±å…¥ç†è§£å±‚æ•°å˜åŒ–å¯¹ç‰¹å¾æå–å’Œå¬å›ç‡çš„å› æœå½±å“")
    print("å®éªŒèŒƒå›´: 1-20å±‚å®Œæ•´æµ‹è¯•")

    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    experiment = ComprehensiveLayersAnalysisExperiment()

    if not os.path.exists(experiment.base_script):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°ä¸»ç¨‹åºæ–‡ä»¶ {experiment.base_script}")
        return

    if not os.path.exists(experiment.base_types):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°ç±»å‹å®šä¹‰æ–‡ä»¶ {experiment.base_types}")
        return

    print(f"\nå®éªŒè®¾è®¡:")
    print(f"  å±‚æ•°èŒƒå›´: 1-20å±‚")
    print(f"  å®éªŒæ•°é‡: {len(experiment.experiments)}")
    print(f"  åˆ†æç»´åº¦: è½®å»“ç»Ÿè®¡ã€å‡ ä½•ç‰¹å¾ã€æ£€ç´¢é”®ã€BCIç‰¹å¾ã€ç›¸ä¼¼æ€§æ£€æŸ¥")
    print(f"  é¢„è®¡æ—¶é•¿: {len(experiment.experiments) * 15} åˆ†é’Ÿ")

    # è¯¢é—®ç”¨æˆ·ç¡®è®¤
    confirm = input(f"\nç»§ç»­è¿è¡Œç»¼åˆåˆ†æå®éªŒ? (y/n): ")
    if confirm.lower() != 'y':
        print("å®éªŒå·²å–æ¶ˆ")
        return

    # è¿è¡Œæ‰€æœ‰å®éªŒ
    experiment.run_all_experiments()

    print("\næ‰€æœ‰ç»¼åˆåˆ†æå®éªŒå®Œæˆ!")
    print("è¯·æŸ¥çœ‹ç”Ÿæˆçš„è¯¦ç»†æŠ¥å‘Šäº†è§£å±‚æ•°å¯¹ç‰¹å¾æå–çš„å…·ä½“å½±å“")


if __name__ == "__main__":
    main()
